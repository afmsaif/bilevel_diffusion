## Reward Fine-Tuning in Diffusion Models (SectionÂ 3.1)

This directory implements the inference-only bilevel approach for reward fine-tuning pre-trained diffusion models as described in SectionÂ 3.1 of **"A First-order Generative Bilevel Optimization Framework for Diffusion Models"** îˆ€fileciteîˆ‚turn0file0îˆ.

### ğŸ“¦ Requirements

* PythonÂ >=Â 3.8
* PyTorchÂ >=Â 1.10
* [diffusers](https://github.com/huggingface/diffusers)
* [transformers](https://github.com/huggingface/transformers)
* [torchvision](https://github.com/pytorch/vision)
* [tqdm](https://github.com/tqdm/tqdm)
* CLIP model (from OpenAI)

Install the required packages:

```bash
pip install -r requirements.txt
```

### ğŸ—‚ï¸ Directory Structure

```
scripts/
â”œâ”€â”€ reward_finetune.py      # Entry-point for bilevel reward fine-tuning
configs/
â”œâ”€â”€ reward_config.yaml      # Default hyperparameters for SectionÂ 3.1
models/
â”œâ”€â”€ resnet18_reward.pth     # Pre-trained ResNetâ€‘18 synthetic reward model
outputs/
â””â”€â”€ reward_finetune/        # Checkpoints and generated samples
```

### ğŸš€ Usage

Run the fine-tuning procedure:

```bash
python scripts/reward_finetune.py \
  --config configs/reward_config.yaml \
  --pretrained_model_path "CompVis/stable-diffusion-v1-5" \
  --lower_reward_model models/resnet18_reward.pth \
  --upper_reward clip \
  --num_samples 256 \
  --lambda_init 0.01 \
  --gamma 1e3 \
  --steps 500 \
  --output_dir outputs/reward_finetune
```

where:

* `--num_samples` is the number of MonteÂ Carlo samples from the pre-trained distribution.
* `--lambda_init` initializes the entropy strength $\lambda$ for the lower-level problem.
* `--gamma` is the penalty constant $\gamma$ controlling approximation accuracy.
* `--steps` is the number of bilevel update iterations for $\lambda$.

### ğŸ” Algorithm Details

SectionÂ 3.1 formulates reward fine-tuning as a bilevel problem:

$$
\min_{\lambda>0,\,p\in S(\lambda)} \; f(\lambda,p) = - \mathbb{E}_{u\sim p}[r_1(u)]
\quad\text{s.t.}\quad
S(\lambda)=\arg\min_{p'}\left\{ -\mathbb{E}_{u\sim p'}[r_2(u)] + \lambda \,\mathrm{KL}(p'\|p_{\mathrm{data}})\right\}
$$

* **Lower level** adjusts sampling via guided backward SDE (AlgorithmÂ 5) with reward $r_2$.
* **Upper level** updates $\lambda$ by MonteÂ Carlo estimation of the closed-form gradient (Eq.Â 13) and projected gradient descent (AlgorithmÂ 2).

### ğŸ“ˆ Output

* Learned entropy strength $\lambda^*$ saved to `outputs/reward_finetune/lambda_final.txt`
* Generated images in `outputs/reward_finetune/samples/`
* Training logs and gradient trajectories in `outputs/reward_finetune/logs/`

### ğŸ“ Citation

If you use this code, please cite:

> QuanÂ Xiao, HuiÂ Yuan, A.Â F.Â M.Â Saif, GaowenÂ Liu, RamanaÂ Kompella, MengdiÂ Wang, TianyiÂ Chen. "A First-order Generative Bilevel Optimization Framework for Diffusion Models." ICMLÂ 2025.
