## Reward Fine-Tuning in Diffusion Models (SectionÂ 3.1)

This directory implements the inference-only bilevel approach for reward fine-tuning in diffusion models as described in SectionÂ 3.1 of **"A First-order Generative Bilevel Optimization Framework for Diffusion Models"**.

### ğŸ“¦ Requirements

* PythonÂ >=Â 3.8
* PyTorchÂ >=Â 1.10
* [diffusers](https://github.com/huggingface/diffusers)
* [transformers](https://github.com/huggingface/transformers)
* [torchvision](https://github.com/pytorch/vision)
* [tqdm](https://github.com/tqdm/tqdm)
* CLIP model (from OpenAI)

Install the required packages:

```bash
pip install -r requirements.txt
```

### ğŸ—‚ï¸ Directory Structure

```
scripts/
â”œâ”€â”€ main.py      # Training for bilevel reward fine-tuning

â”œâ”€â”€ reward_config.yaml      # Default hyperparameters for SectionÂ 3.1

â”œâ”€â”€ scorer_bi.py     # Reward functions

â””â”€â”€ gradguided_sdpipeline.py/        # Checkpoints and generated samples
```

### ğŸš€ Usage

Run the fine-tuning procedure:

```bash
python scripts/reward_finetune.py \
  --config configs/reward_config.yaml \
  --pretrained_model_path "CompVis/stable-diffusion-v1-5" \
  --lower_reward_model models/resnet18_reward.pth \
  --upper_reward clip \
  --num_samples 256 \
  --lambda_init 0.01 \
  --gamma 1e3 \
  --steps 500 \
  --output_dir outputs/reward_finetune
```

where:

* `--num_samples` is the number of MonteÂ Carlo samples from the pre-trained distribution.
* `--lambda_init` initializes the entropy strength $\lambda$ for the lower-level problem.
* `--gamma` is the penalty constant $\gamma$ controlling approximation accuracy.
* `--steps` is the number of bilevel update iterations for $\lambda$.

### ğŸ” Algorithm Details

SectionÂ 3.1 formulates reward fine-tuning as a bilevel problem:

<pre> Minimize over Î» âˆˆ â„â‚Š and p âˆˆ ğ’®(Î»): f(Î», p) := â€“E_{u ~ p}[râ‚(u)] Subject to: ğ’®(Î») = argmin_{pâ€² âˆˆ ğ’«} { â€“E_{u ~ pâ€²}[râ‚‚(u)] + Î» Â· KL(pâ€² || p_data) } </pre>

* **Lower level** adjusts sampling via guided backward SDE (AlgorithmÂ 5) with entropy regularized reward $r_2$.
* **Upper level** updates $\lambda$ by MonteÂ Carlo estimation of the closed-form gradient (Eq.Â 13) and projected gradient descent (AlgorithmÂ 2).

### ğŸ“ˆ Output

* Learned entropy strength $\lambda^*$ saved to `outputs/reward_finetune/lambda_final.txt`
* Generated images in `outputs/reward_finetune/samples/`
* Training logs and gradient trajectories in `outputs/reward_finetune/logs/`

### ğŸ“ Citation

If you use this code, please cite:

```
@inproceedings{xiao2025first,
  title={A First-order Generative Bilevel Optimization Framework for Diffusion Models},
  author={Xiao, Quan and Yuan, Hui and Saif, AFM and Liu, Gaowen and Kompella, Ramana and Wang, Mengdi and Chen, Tianyi},
  booktitle={Proceedings of the 42nd International Conference on Machine Learning},
  year={2025}
}
```

